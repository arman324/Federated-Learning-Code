{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d3e12bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tenseal as ts \n",
    "import base64\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4e861d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"mnist.pkl.gz\"\n",
    "DATA_PATH = Path(\"/Users/tung/Desktop/Federated-Learning-Code\")\n",
    "PATH = DATA_PATH / \"Downloads\"\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a5fad2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "09532479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "28a11a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8c688a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "965fa7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d40eb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function compares the accuracy of the main model \n",
    "# and the local model running on each node.\n",
    "def compare_local_and_merged_model_performance(number_of_samples):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_samples,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_samples):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5860d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Federated Averaging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2f3e1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers are algorithms or methods used to minimize an\n",
    "# error function(loss function)or to maximize the efficiency of production. \n",
    "\n",
    "# This function creates a model, optimizer and loss function for each node.\n",
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3df753ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the average of the weights in individual nodes.\n",
    "\n",
    "def get_averaged_weights(model_dict, number_of_samples):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_samples):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_samples\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_samples\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_samples\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_samples\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_samples\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_samples\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2c9bb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sends the averaged weights of individual nodes \n",
    "# to the main model and sets them as the new weights of the main model. ( calls def get_averaged_weights(model_dict, number_of_samples))\n",
    "\n",
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_samples=number_of_samples)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4401ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sends the parameters of the main model to the nodes.\n",
    "# The following data should send from SERVER to each node\n",
    "# So we should save them on a file\n",
    "\n",
    "def send_main_model_to_nodes_and_update_model_dict(main_model,want_print):\n",
    "    with torch.no_grad():\n",
    "            torch.save(main_model.fc1.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc1_weight_data.pt')\n",
    "            torch.save(main_model.fc2.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc2_weight_data.pt')\n",
    "            torch.save(main_model.fc3.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc3_weight_data.pt')\n",
    "            \n",
    "            torch.save(main_model.fc1.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc1_bias_data.pt')\n",
    "            torch.save(main_model.fc2.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc2_bias_data.pt')\n",
    "            torch.save(main_model.fc3.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc3_bias_data.pt')\n",
    "\n",
    "            if (want_print == 1):\n",
    "                print(main_model.fc1.weight.data.clone())\n",
    "                print(\"--------------------------------------------------------\")\n",
    "                print(main_model.fc2.weight.data.clone())\n",
    "                print(\"--------------------------------------------------------\")\n",
    "                print(main_model.fc3.weight.data.clone())\n",
    "                print(\"--------------------------------------------------------\")\n",
    "\n",
    "                print(main_model.fc1.bias.data.clone())\n",
    "                print(\"--------------------------------------------------------\")\n",
    "                print(main_model.fc2.bias.data.clone())\n",
    "                print(\"--------------------------------------------------------\")\n",
    "                print(main_model.fc3.bias.data.clone()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2d811fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function downloads all of the local models\n",
    "\n",
    "def download_local_model_from_each_node(number_of_samples):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_samples):\n",
    "            model_dict[name_of_models[i]].fc1.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc1_weight_data.pt')\n",
    "            model_dict[name_of_models[i]].fc2.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc2_weight_data.pt')\n",
    "            model_dict[name_of_models[i]].fc3.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc3_weight_data.pt')\n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc1_bias_data.pt')\n",
    "            model_dict[name_of_models[i]].fc2.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc2_bias_data.pt')\n",
    "            model_dict[name_of_models[i]].fc3.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc3_bias_data.pt')\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "19c19ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n",
    "number_of_samples = 3\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "\n",
    "train_amount = 4500\n",
    "valid_amount = 900\n",
    "test_amount = 900\n",
    "print_amount = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9a88616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model is created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d9c7e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Net2nn()\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7848ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0218, -0.0264, -0.0285,  0.0603, -0.0585]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b086a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0123,  0.0187, -0.0200,  ..., -0.0011,  0.0168, -0.0340],\n",
      "        [-0.0323, -0.0169, -0.0030,  ...,  0.0045,  0.0309, -0.0146],\n",
      "        [-0.0325,  0.0177, -0.0270,  ...,  0.0311, -0.0093,  0.0153],\n",
      "        ...,\n",
      "        [-0.0266, -0.0231, -0.0240,  ..., -0.0313, -0.0312,  0.0078],\n",
      "        [-0.0037, -0.0078, -0.0010,  ..., -0.0172,  0.0165,  0.0230],\n",
      "        [-0.0031, -0.0179, -0.0066,  ..., -0.0160, -0.0094, -0.0290]])\n",
      "--------------------------------------------------------\n",
      "tensor([[ 0.0218, -0.0264, -0.0285,  ...,  0.0584, -0.0221, -0.0628],\n",
      "        [ 0.0218,  0.0574,  0.0134,  ...,  0.0011,  0.0649, -0.0144],\n",
      "        [ 0.0654,  0.0384, -0.0358,  ..., -0.0506, -0.0685, -0.0200],\n",
      "        ...,\n",
      "        [-0.0113,  0.0007, -0.0278,  ..., -0.0212,  0.0333,  0.0613],\n",
      "        [ 0.0696,  0.0605,  0.0611,  ...,  0.0339,  0.0118, -0.0580],\n",
      "        [-0.0394,  0.0360,  0.0259,  ..., -0.0562,  0.0689, -0.0594]])\n",
      "--------------------------------------------------------\n",
      "tensor([[-6.7932e-02,  3.1851e-02, -3.1614e-02,  ...,  3.9154e-02,\n",
      "         -5.3283e-02, -3.7764e-02],\n",
      "        [ 4.8948e-03, -1.1141e-02,  6.9089e-02,  ...,  2.7684e-02,\n",
      "          3.1388e-02, -5.8907e-02],\n",
      "        [-4.6509e-02,  3.7402e-02, -2.7986e-02,  ...,  5.5135e-02,\n",
      "          1.1777e-02,  7.4525e-03],\n",
      "        ...,\n",
      "        [-2.8279e-02, -1.4243e-03,  4.4479e-02,  ..., -1.8175e-02,\n",
      "          6.3445e-03,  1.6160e-05],\n",
      "        [ 5.7502e-02,  4.6720e-02,  1.6573e-02,  ...,  2.4838e-02,\n",
      "          6.0024e-02, -3.1446e-03],\n",
      "        [ 1.8892e-02, -5.0013e-03,  6.1450e-02,  ...,  6.5784e-02,\n",
      "         -2.2116e-02,  6.7715e-02]])\n",
      "--------------------------------------------------------\n",
      "tensor([-0.0276, -0.0340,  0.0251, -0.0232, -0.0345,  0.0092, -0.0223, -0.0161,\n",
      "        -0.0195,  0.0191,  0.0325, -0.0110, -0.0281, -0.0246, -0.0247, -0.0005,\n",
      "         0.0349,  0.0008, -0.0199,  0.0192,  0.0183, -0.0345,  0.0275, -0.0137,\n",
      "        -0.0012, -0.0125,  0.0074,  0.0326, -0.0115, -0.0031,  0.0164, -0.0103,\n",
      "        -0.0145,  0.0353, -0.0024,  0.0008, -0.0283, -0.0128,  0.0356, -0.0112,\n",
      "         0.0292, -0.0170, -0.0263, -0.0158,  0.0290, -0.0259, -0.0170, -0.0060,\n",
      "         0.0047, -0.0179,  0.0056,  0.0203, -0.0005, -0.0220, -0.0099,  0.0225,\n",
      "        -0.0101, -0.0091, -0.0109, -0.0251, -0.0003,  0.0352, -0.0258,  0.0327,\n",
      "         0.0186, -0.0311, -0.0013,  0.0244,  0.0160, -0.0070,  0.0193,  0.0053,\n",
      "         0.0284,  0.0063, -0.0282, -0.0067, -0.0343,  0.0058,  0.0116, -0.0196,\n",
      "        -0.0228, -0.0244,  0.0028, -0.0029,  0.0205,  0.0036, -0.0287,  0.0288,\n",
      "         0.0059, -0.0330,  0.0314,  0.0144, -0.0083, -0.0112, -0.0026,  0.0260,\n",
      "        -0.0180,  0.0145,  0.0189, -0.0023,  0.0154, -0.0188,  0.0341,  0.0168,\n",
      "        -0.0149, -0.0069,  0.0269,  0.0342, -0.0170, -0.0056, -0.0280,  0.0140,\n",
      "         0.0262,  0.0291, -0.0058,  0.0051,  0.0181, -0.0118,  0.0205,  0.0296,\n",
      "        -0.0247, -0.0144, -0.0256,  0.0017, -0.0041,  0.0352,  0.0063,  0.0031,\n",
      "        -0.0067,  0.0340,  0.0260, -0.0088,  0.0157,  0.0041, -0.0200, -0.0267,\n",
      "        -0.0001, -0.0081, -0.0014,  0.0077, -0.0140, -0.0108, -0.0145, -0.0059,\n",
      "        -0.0108,  0.0212,  0.0159, -0.0153,  0.0205, -0.0242, -0.0204,  0.0046,\n",
      "         0.0230, -0.0170, -0.0083, -0.0168, -0.0108,  0.0300,  0.0290,  0.0323,\n",
      "        -0.0075,  0.0133,  0.0299,  0.0300,  0.0334,  0.0212, -0.0082,  0.0285,\n",
      "        -0.0132,  0.0012, -0.0246,  0.0199,  0.0222, -0.0305, -0.0179,  0.0184,\n",
      "        -0.0092,  0.0018, -0.0227, -0.0264,  0.0030, -0.0013, -0.0230, -0.0217,\n",
      "        -0.0193, -0.0322, -0.0109, -0.0230,  0.0069, -0.0115, -0.0205,  0.0078,\n",
      "        -0.0112, -0.0128, -0.0143,  0.0242,  0.0208, -0.0190,  0.0089,  0.0261])\n",
      "--------------------------------------------------------\n",
      "tensor([ 2.5814e-02, -7.0457e-03, -6.5978e-02, -5.3352e-02, -2.5461e-02,\n",
      "         3.6050e-02,  2.3489e-02, -3.8823e-02,  4.8182e-02,  1.4422e-02,\n",
      "        -1.4717e-02,  6.3679e-02, -5.8148e-02, -2.6823e-02, -3.5458e-02,\n",
      "         1.8018e-02, -6.0876e-02, -3.2270e-02,  2.9630e-02, -2.2287e-02,\n",
      "         2.9010e-02,  6.3215e-02, -3.5427e-02, -3.5478e-02, -5.0615e-02,\n",
      "        -1.5556e-02,  5.5500e-02,  8.0640e-03, -6.6548e-02, -6.0406e-02,\n",
      "         6.8971e-02,  2.9002e-02, -4.3999e-02, -3.4133e-02, -1.5084e-02,\n",
      "        -5.4731e-02,  1.7957e-02, -3.7575e-02, -1.4306e-02,  6.1469e-02,\n",
      "        -4.7057e-02,  4.4291e-02,  2.5721e-02,  5.9251e-02,  2.8713e-02,\n",
      "        -2.5713e-02, -3.1691e-02,  4.9205e-02,  1.8539e-02, -4.1838e-02,\n",
      "         5.3542e-03,  6.9918e-02,  3.3267e-03,  5.9369e-02,  6.1759e-02,\n",
      "         3.9930e-02,  2.1402e-02,  3.5545e-02,  1.2422e-02, -8.7210e-03,\n",
      "         5.8739e-02,  2.6443e-02,  4.0103e-02,  2.5271e-02, -3.0562e-02,\n",
      "        -6.0667e-02, -6.6347e-03, -6.8873e-02, -9.6565e-03, -4.2244e-02,\n",
      "        -6.3280e-02, -3.3063e-02, -1.9314e-02,  3.4744e-02, -9.3144e-03,\n",
      "         2.5040e-02, -6.8070e-02,  4.3915e-02, -4.9036e-02, -6.0318e-02,\n",
      "        -4.3347e-05,  4.7964e-03, -5.6757e-02, -5.7126e-02, -4.2268e-02,\n",
      "        -6.4200e-02, -4.7299e-02,  3.0880e-04, -6.7417e-02,  5.6805e-02,\n",
      "         1.3969e-02,  7.7750e-03, -8.9863e-03, -2.5947e-02,  4.7166e-02,\n",
      "         4.2609e-02,  2.1346e-02,  4.7996e-02,  6.7698e-03, -7.0260e-02,\n",
      "         3.5078e-02,  2.0154e-02,  6.4927e-02,  1.9833e-02,  2.5812e-02,\n",
      "        -1.1955e-02, -2.3267e-02, -6.0429e-02,  2.1218e-02,  1.9368e-02,\n",
      "         3.2708e-02,  4.4057e-02,  5.8452e-02, -4.3120e-02,  3.1089e-02,\n",
      "        -3.7968e-02, -2.1024e-02,  1.1606e-02, -3.3389e-02, -6.3242e-02,\n",
      "        -4.9193e-02,  5.9790e-02,  5.3481e-02,  4.0728e-02, -5.3230e-02,\n",
      "         3.8653e-02,  5.2910e-02,  4.4753e-02, -5.0018e-03, -1.5714e-02,\n",
      "         2.6912e-02,  4.0519e-02,  4.6030e-02, -4.0612e-02,  2.4306e-02,\n",
      "        -3.6683e-02, -4.4723e-02, -6.1494e-02,  4.8612e-02,  3.3940e-02,\n",
      "         6.9002e-02,  3.5059e-03,  5.9176e-02, -1.8336e-02,  5.7792e-02,\n",
      "         1.7265e-02, -3.5260e-03, -5.5210e-02, -2.3956e-02,  3.7710e-02,\n",
      "        -2.0828e-02, -6.5880e-03, -6.7030e-03,  3.0227e-02, -6.2977e-02,\n",
      "        -9.9365e-03, -6.4715e-02,  4.2395e-02, -1.2993e-03, -4.4068e-02,\n",
      "         2.1110e-02, -3.1197e-02, -5.1727e-02,  5.1587e-02, -3.0943e-02,\n",
      "        -1.3692e-02, -1.8664e-02,  1.8318e-02, -3.2380e-02, -5.0209e-02,\n",
      "        -3.3898e-02, -3.2776e-02,  5.3156e-02,  4.0848e-03, -1.5702e-02,\n",
      "         2.9173e-02,  3.4588e-02, -1.3322e-02,  4.7864e-02, -6.9513e-02,\n",
      "        -1.2947e-03, -3.7213e-02, -2.3796e-02,  3.8648e-03,  5.7354e-02,\n",
      "         3.6641e-02, -6.8094e-02,  4.7224e-02,  2.5218e-02, -6.7229e-02,\n",
      "         4.4501e-02, -3.6538e-02,  3.8081e-02,  3.6679e-02,  2.4873e-02,\n",
      "        -4.7882e-02,  6.3011e-02, -1.3919e-02,  1.7465e-02, -5.7558e-02])\n",
      "--------------------------------------------------------\n",
      "tensor([ 0.0448,  0.0496, -0.0029, -0.0547,  0.0436, -0.0544, -0.0578, -0.0097,\n",
      "        -0.0084,  0.0362])\n"
     ]
    }
   ],
   "source": [
    "# The following data should send from SERVER to each node\n",
    "# So we should save them on a file\n",
    "\n",
    "send_main_model_to_nodes_and_update_model_dict(main_model,1) # if 1, then print weight and bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "03cf0cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0218, -0.0264, -0.0285,  0.0603, -0.0585]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "92a8eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict()\n",
    "for i in range(number_of_samples):\n",
    "    model_name=\"model\"+str(i)\n",
    "    model_info=Net2nn()\n",
    "    model_dict.update({model_name : model_info })\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f02ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the first time run, we should run the following code\n",
    "\n",
    "\n",
    "# This function downloads all of the local models\n",
    "# We should first run the client code. Then here.\n",
    "# model_dict = download_local_model_from_each_node(number_of_samples)\n",
    "# print(model_dict[\"model1\"].fc2.weight[0,0:5])\n",
    "# print(model_dict[\"model0\"].fc2.weight[0,0:5])\n",
    "# print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# main_model = set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples) \n",
    "\n",
    "# print(main_model.fc2.weight[0:1,0:5])\n",
    "# print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# train_ds = TensorDataset(x_train, y_train)\n",
    "# train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# valid_ds = TensorDataset(x_valid, y_valid)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# test_ds = TensorDataset(x_test, y_test)\n",
    "# test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "\n",
    "# test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "\n",
    "\n",
    "\n",
    "# test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "# print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))\n",
    "# print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "# send_main_model_to_nodes_and_update_model_dict(main_model,0) # if 1, then print weight and bias\n",
    "# For storing public and private keys\n",
    "def write_data(file_name, data):\n",
    "    if type(data) == bytes:\n",
    "        #bytes to base64\n",
    "        data = base64.b64encode(data)\n",
    "         \n",
    "    with open(file_name, 'wb') as f: \n",
    "        f.write(data)\n",
    " \n",
    "def read_data(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    #base64 to bytes\n",
    "    return base64.b64decode(data)\n",
    "\n",
    "context = ts.context_from(read_data(\"public.txt\"))\n",
    "\n",
    "# Load local parameters and compute average\n",
    "for i in range(200):\n",
    "    # FC1\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_0_1_weight_\" + str(i))\n",
    "    encrypted_model_0_1_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_0_1_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_1_1_weight_\" + str(i))\n",
    "    encrypted_model_1_1_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_1_1_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_2_1_weight_\" + str(i))\n",
    "    encrypted_model_2_1_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_2_1_weight.link_context(context)\n",
    "\n",
    "    average_encrypted_model_weight = (encrypted_model_0_1_weight + encrypted_model_1_1_weight + encrypted_model_2_1_weight) * 0.333333333\n",
    "    write_data('Main_Model_Parameters/encrypted_model_1_weight_' + str(i), average_encrypted_model_weight.serialize())\n",
    "\n",
    "    # FC2\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_0_2_weight_\" + str(i))\n",
    "    encrypted_model_0_2_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_0_2_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_1_2_weight_\" + str(i))\n",
    "    encrypted_model_1_2_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_1_2_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_2_2_weight_\" + str(i))\n",
    "    encrypted_model_2_2_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_2_2_weight.link_context(context)\n",
    "\n",
    "    average_encrypted_model_weight = (encrypted_model_0_2_weight + encrypted_model_1_2_weight + encrypted_model_2_2_weight) * 0.333333333\n",
    "    write_data('Main_Model_Parameters/encrypted_model_2_weight_' + str(i), average_encrypted_model_weight.serialize())\n",
    "\n",
    "for i in range(10):\n",
    "    # FC3\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_0_3_weight_\" + str(i))\n",
    "    encrypted_model_0_3_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_0_3_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_1_3_weight_\" + str(i))\n",
    "    encrypted_model_1_3_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_1_3_weight.link_context(context)\n",
    "\n",
    "    temp = read_data(\"Local_Model_Parameters/encrypted_model_2_3_weight_\" + str(i))\n",
    "    encrypted_model_2_3_weight = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_2_3_weight.link_context(context)\n",
    "\n",
    "    average_encrypted_model_weight = (encrypted_model_0_3_weight + encrypted_model_1_3_weight + encrypted_model_2_3_weight) * 0.333333333\n",
    "    write_data('Main_Model_Parameters/encrypted_model_3_weight_' + str(i), average_encrypted_model_weight.serialize())\n",
    "\n",
    "# Bias FC1\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_0_1_bias\")\n",
    "encrypted_model_0_1_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_0_1_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_1_1_bias\")\n",
    "encrypted_model_1_1_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_1_1_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_2_1_bias\")\n",
    "encrypted_model_2_1_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_2_1_bias.link_context(context)\n",
    "\n",
    "average_encrypted_model_bias = (encrypted_model_0_1_bias + encrypted_model_1_1_bias + encrypted_model_2_1_bias) * 0.333333333\n",
    "write_data('Main_Model_Parameters/encrypted_model_1_bias', average_encrypted_model_bias.serialize())\n",
    "\n",
    "# Bias FC2\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_0_2_bias\")\n",
    "encrypted_model_0_2_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_0_2_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_1_2_bias\")\n",
    "encrypted_model_1_2_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_1_2_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_2_2_bias\")\n",
    "encrypted_model_2_2_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_2_2_bias.link_context(context)\n",
    "\n",
    "average_encrypted_model_bias = (encrypted_model_0_2_bias + encrypted_model_1_2_bias + encrypted_model_2_2_bias) * 0.333333333\n",
    "write_data('Main_Model_Parameters/encrypted_model_2_bias', average_encrypted_model_bias.serialize())\n",
    "\n",
    "# Bias FC3\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_0_3_bias\")\n",
    "encrypted_model_0_3_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_0_3_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_1_3_bias\")\n",
    "encrypted_model_1_3_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_1_3_bias.link_context(context)\n",
    "\n",
    "temp = read_data(\"Local_Model_Parameters/encrypted_model_2_3_bias\")\n",
    "encrypted_model_2_3_bias = ts.lazy_ckks_vector_from(temp)\n",
    "encrypted_model_2_3_bias.link_context(context)\n",
    "\n",
    "average_encrypted_model_bias = (encrypted_model_0_3_bias + encrypted_model_1_3_bias + encrypted_model_2_3_bias) * 0.333333333\n",
    "write_data('Main_Model_Parameters/encrypted_model_3_bias', average_encrypted_model_bias.serialize())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
