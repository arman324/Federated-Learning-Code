{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97045d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tenseal as ts \n",
    "from os.path import exists\n",
    "import base64\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950acc8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07539a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"mnist.pkl.gz\"\n",
    "DATA_PATH = Path(\"./\")\n",
    "PATH = DATA_PATH \n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "# For homomorphic encryption\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree = 8192, coeff_mod_bit_sizes = [60, 40, 40, 60])\n",
    "context.generate_galois_keys()\n",
    "context.global_scale = 2**40\n",
    "\n",
    "# For storing public and private keys\n",
    "def write_data(file_name, data):\n",
    "    if type(data) == bytes:\n",
    "        #bytes to base64\n",
    "        data = base64.b64encode(data)\n",
    "         \n",
    "    with open(file_name, 'wb') as f: \n",
    "        f.write(data)\n",
    " \n",
    "def read_data(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    #base64 to bytes\n",
    "    return base64.b64decode(data)\n",
    "\n",
    "if exists(\"./secret.txt\") & exists(\"./public.txt\"):\n",
    "    public_context = ts.context_from(read_data(\"public.txt\"))\n",
    "    secret_context = ts.context_from(read_data(\"secret.txt\"))\n",
    "else:\n",
    "    # Store public and private keys\n",
    "    secret_context = context.serialize(save_secret_key = True)\n",
    "    write_data(\"secret.txt\", secret_context)\n",
    "    \n",
    "    context.make_context_public() #drop the secret_key from the context\n",
    "    public_context = context.serialize()\n",
    "    write_data(\"public.txt\", public_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a44be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_shuffle_labels(y_data, seed, amount):\n",
    "    y_data=pd.DataFrame(y_data,columns=[\"labels\"])\n",
    "    y_data[\"i\"]=np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    for i in range(10):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i]\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=label_info[0:amount]\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "520e3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iid_subsamples_indices(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for j in range(10):\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc3c0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"]))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76badd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4454ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59bf3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b7b9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91ce84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cf95ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers are algorithms or methods used to minimize an\n",
    "# error function(loss function)or to maximize the efficiency of production. \n",
    "\n",
    "# This function creates a model, optimizer and loss function for each node.\n",
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "        print(\"MYYYY MODELLL DICT\", model_dict)\n",
    "    return model_dict, optimizer_dict, criterion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbb0c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should replace the main_model parameters to each local_model parameters\n",
    "def download_main_model_for_each_node(model_dict, number_of_samples):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_samples):\n",
    "            model_dict[name_of_models[i]].fc1.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc1_weight_data.pt')\n",
    "            model_dict[name_of_models[i]].fc2.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc2_weight_data.pt')\n",
    "            model_dict[name_of_models[i]].fc3.weight.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc3_weight_data.pt')\n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc1_bias_data.pt')\n",
    "            model_dict[name_of_models[i]].fc2.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc2_bias_data.pt')\n",
    "            model_dict[name_of_models[i]].fc3.bias.data = torch.load('/Users/tung/Desktop/Federated-Learning-Code/Main_Model_Parameters/main_model_fc3_bias_data.pt')\n",
    "    return model_dict \n",
    "\n",
    "def download_encrypted_average_main_model(model_dict, number_of_samples):\n",
    "    # Clients has the secret key\n",
    "    context = ts.context_from(read_data(\"secret.txt\"))\n",
    "    \n",
    "    for i in range(200):\n",
    "        temp = read_data(\"Main_Model_Parameters/encrypted_model_1_weight_\" + str(i))\n",
    "        encrypted_model_1_weight = ts.lazy_ckks_vector_from(temp)\n",
    "        encrypted_model_1_weight.link_context(context)\n",
    "        model_dict[\"model0\"].fc1.weight[i,:].data[:] = torch.as_tensor(encrypted_model_1_weight.decrypt())\n",
    "        model_dict[\"model1\"].fc1.weight[i,:].data[:] = torch.as_tensor(encrypted_model_1_weight.decrypt())\n",
    "        model_dict[\"model2\"].fc1.weight[i,:].data[:] = torch.as_tensor(encrypted_model_1_weight.decrypt())\n",
    "\n",
    "        temp = read_data(\"Main_Model_Parameters/encrypted_model_2_weight_\" + str(i))\n",
    "        encrypted_model_2_weight = ts.lazy_ckks_vector_from(temp)\n",
    "        encrypted_model_2_weight.link_context(context)\n",
    "        model_dict[\"model0\"].fc2.weight[i,:].data[:] = torch.tensor(encrypted_model_2_weight.decrypt())\n",
    "        model_dict[\"model1\"].fc2.weight[i,:].data[:] = torch.tensor(encrypted_model_2_weight.decrypt())\n",
    "        model_dict[\"model2\"].fc2.weight[i,:].data[:] = torch.tensor(encrypted_model_2_weight.decrypt())\n",
    "        \n",
    "    for i in range(10):\n",
    "        temp = read_data(\"Main_Model_Parameters/encrypted_model_3_weight_\" + str(i))\n",
    "        encrypted_model_3_weight = ts.lazy_ckks_vector_from(temp)\n",
    "        encrypted_model_3_weight.link_context(context)\n",
    "        model_dict[\"model0\"].fc3.weight[i,:].data[:] = torch.tensor(encrypted_model_3_weight.decrypt())\n",
    "        model_dict[\"model1\"].fc3.weight[i,:].data[:] = torch.tensor(encrypted_model_3_weight.decrypt())\n",
    "        model_dict[\"model2\"].fc3.weight[i,:].data[:] = torch.tensor(encrypted_model_3_weight.decrypt())\n",
    "\n",
    "    temp = read_data(\"Main_Model_Parameters/encrypted_model_1_bias\")\n",
    "    encrypted_model_1_bias = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_1_bias.link_context(context)\n",
    "    model_dict[\"model0\"].fc1.bias.data[:] = torch.as_tensor(encrypted_model_1_bias.decrypt())\n",
    "    model_dict[\"model1\"].fc1.bias.data[:] = torch.as_tensor(encrypted_model_1_bias.decrypt())\n",
    "    model_dict[\"model2\"].fc1.bias.data[:] = torch.as_tensor(encrypted_model_1_bias.decrypt())\n",
    "\n",
    "    temp = read_data(\"Main_Model_Parameters/encrypted_model_2_bias\")\n",
    "    encrypted_model_2_bias = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_2_bias.link_context(context)\n",
    "    model_dict[\"model0\"].fc2.bias.data[:] = torch.as_tensor(encrypted_model_2_bias.decrypt())\n",
    "    model_dict[\"model1\"].fc2.bias.data[:] = torch.as_tensor(encrypted_model_2_bias.decrypt())\n",
    "    model_dict[\"model2\"].fc2.bias.data[:] = torch.as_tensor(encrypted_model_2_bias.decrypt())\n",
    "\n",
    "    temp = read_data(\"Main_Model_Parameters/encrypted_model_3_bias\")\n",
    "    encrypted_model_3_bias = ts.lazy_ckks_vector_from(temp)\n",
    "    encrypted_model_3_bias.link_context(context)\n",
    "    model_dict[\"model0\"].fc3.bias.data[:] = torch.as_tensor(encrypted_model_3_bias.decrypt())\n",
    "    model_dict[\"model1\"].fc3.bias.data[:] = torch.as_tensor(encrypted_model_3_bias.decrypt())\n",
    "    model_dict[\"model2\"].fc3.bias.data[:] = torch.as_tensor(encrypted_model_3_bias.decrypt())\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a6b8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains individual local models in nodes.\n",
    "def start_train_end_node_process(number_of_samples):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n",
    "#         valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "        \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        print(\"Subset\" ,i)\n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "#             valid_loss, valid_accuracy = validation(model, valid_dl, criterion)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "    \n",
    "            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d7d5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains individual local models in nodes.\n",
    "def start_train_end_node_process_without_print(number_of_samples):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b438f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains individual local models in nodes.\n",
    "def start_train_end_node_process_print_some(number_of_samples, print_amount):\n",
    "    for i in range (number_of_samples): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42d07c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sends the parameters of the each local_model to the SERVER.\n",
    "# So we should save them on a file\n",
    "\n",
    "def send_data_from_local_models_to_server(model_dict, number_of_samples):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_samples):\n",
    "            torch.save(model_dict[name_of_models[i]].fc1.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc1_weight_data.pt')\n",
    "            torch.save(model_dict[name_of_models[i]].fc1.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc1_bias_data.pt')\n",
    "\n",
    "            torch.save(model_dict[name_of_models[i]].fc2.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc2_weight_data.pt')\n",
    "            torch.save(model_dict[name_of_models[i]].fc2.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc2_bias_data.pt')\n",
    "\n",
    "            torch.save(model_dict[name_of_models[i]].fc3.weight.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc3_weight_data.pt')\n",
    "            torch.save(model_dict[name_of_models[i]].fc3.bias.data.clone(), '/Users/tung/Desktop/Federated-Learning-Code/Local_Model_Parameters/local_model_'+str(i)+'_fc3_bias_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfaeeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid,x_test, y_test = map(torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test))\n",
    "number_of_samples = 3\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "\n",
    "train_amount = 4500\n",
    "valid_amount = 900\n",
    "test_amount = 900\n",
    "print_amount = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "561097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is distributed to nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc90731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "sample_dict_train=get_iid_subsamples_indices(label_dict=label_dict_train, number_of_samples=number_of_samples, amount=train_amount)\n",
    "x_train_dict, y_train_dict = create_iid_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")\n",
    "\n",
    "label_dict_valid = split_and_shuffle_labels(y_data=y_valid, seed=1, amount=train_amount) \n",
    "sample_dict_valid = get_iid_subsamples_indices(label_dict=label_dict_valid, number_of_samples=number_of_samples, amount=valid_amount)\n",
    "x_valid_dict, y_valid_dict = create_iid_subsamples(sample_dict=sample_dict_valid, x_data=x_valid, y_data=y_valid, x_name=\"x_valid\", y_name=\"y_valid\")\n",
    "\n",
    "label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=1, amount=test_amount) \n",
    "sample_dict_test = get_iid_subsamples_indices(label_dict=label_dict_test, number_of_samples=number_of_samples, amount=test_amount)\n",
    "x_test_dict, y_test_dict = create_iid_subsamples(sample_dict=sample_dict_test, x_data=x_test, y_data=y_test, x_name=\"x_test\", y_name=\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4520519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models,optimizers and loss functions in nodes are defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "075acc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYYYY MODELLL DICT {'model0': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")}\n",
      "MYYYY MODELLL DICT {'model0': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      "), 'model1': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")}\n",
      "MYYYY MODELLL DICT {'model0': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      "), 'model1': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      "), 'model2': Net2nn(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84db34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys of dicts are being made iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a129f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "name_of_x_valid_sets=list(x_valid_dict.keys())\n",
    "name_of_y_valid_sets=list(y_valid_dict.keys())\n",
    "name_of_x_test_sets=list(x_test_dict.keys())\n",
    "name_of_y_test_sets=list(y_test_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n",
    "\n",
    "# print(name_of_x_train_sets)\n",
    "# print(name_of_y_train_sets)\n",
    "# print(name_of_x_valid_sets)\n",
    "# print(name_of_y_valid_sets)\n",
    "# print(name_of_x_test_sets)\n",
    "# print(name_of_y_test_sets)\n",
    "# print(\"\\n ------------\")\n",
    "# print(name_of_models)\n",
    "# print(name_of_optimizers)\n",
    "# print(name_of_criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53a53381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0447, -0.0437, -0.0335, -0.0376, -0.0611]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0518,  0.0474,  0.0405,  0.0644, -0.0257]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model0\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c454c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of main_model should be downloaded by each node\n",
    "model_dict = download_main_model_for_each_node(model_dict, number_of_samples)\n",
    "\n",
    "# model_dict = download_encrypted_average_main_model(model_dict, number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae82bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0616,  0.0281, -0.0098, -0.0555,  0.0535]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0616,  0.0281, -0.0098, -0.0555,  0.0535]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model0\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03aca740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in the nodes are trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6908faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 0\n",
      "epoch:   1 | train accuracy: 0.76927 | test accuracy: 0.89700\n",
      "epoch:   2 | train accuracy: 0.91207 | test accuracy: 0.92567\n",
      "epoch:   3 | train accuracy: 0.93720 | test accuracy: 0.93467\n",
      "epoch:   4 | train accuracy: 0.95233 | test accuracy: 0.94000\n",
      "epoch:   5 | train accuracy: 0.96433 | test accuracy: 0.94467\n",
      "epoch:   6 | train accuracy: 0.96973 | test accuracy: 0.95667\n",
      "epoch:   7 | train accuracy: 0.97660 | test accuracy: 0.95967\n",
      "epoch:   8 | train accuracy: 0.98080 | test accuracy: 0.95567\n",
      "epoch:   9 | train accuracy: 0.98607 | test accuracy: 0.95967\n",
      "epoch:  10 | train accuracy: 0.98980 | test accuracy: 0.96167\n",
      "Subset 1\n",
      "epoch:   1 | train accuracy: 0.76267 | test accuracy: 0.91233\n",
      "epoch:   2 | train accuracy: 0.91840 | test accuracy: 0.93400\n",
      "epoch:   3 | train accuracy: 0.94027 | test accuracy: 0.94533\n",
      "epoch:   4 | train accuracy: 0.95560 | test accuracy: 0.95000\n",
      "epoch:   5 | train accuracy: 0.96693 | test accuracy: 0.95500\n",
      "epoch:   6 | train accuracy: 0.97140 | test accuracy: 0.95867\n",
      "epoch:   7 | train accuracy: 0.97827 | test accuracy: 0.95933\n",
      "epoch:   8 | train accuracy: 0.98400 | test accuracy: 0.96067\n",
      "epoch:   9 | train accuracy: 0.98793 | test accuracy: 0.95700\n",
      "epoch:  10 | train accuracy: 0.99153 | test accuracy: 0.95900\n",
      "Subset 2\n",
      "epoch:   1 | train accuracy: 0.76527 | test accuracy: 0.91210\n",
      "epoch:   2 | train accuracy: 0.91293 | test accuracy: 0.93483\n",
      "epoch:   3 | train accuracy: 0.93853 | test accuracy: 0.94853\n",
      "epoch:   4 | train accuracy: 0.95347 | test accuracy: 0.94786\n",
      "epoch:   5 | train accuracy: 0.96547 | test accuracy: 0.95388\n",
      "epoch:   6 | train accuracy: 0.97373 | test accuracy: 0.95655\n",
      "epoch:   7 | train accuracy: 0.97933 | test accuracy: 0.95722\n",
      "epoch:   8 | train accuracy: 0.98540 | test accuracy: 0.96156\n",
      "epoch:   9 | train accuracy: 0.98920 | test accuracy: 0.96223\n",
      "epoch:  10 | train accuracy: 0.99087 | test accuracy: 0.95822\n"
     ]
    }
   ],
   "source": [
    "# start_train_end_node_process()\n",
    "start_train_end_node_process_print_some(number_of_samples, print_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "777f69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import float64\n",
    "\n",
    "encrypted_model_0_1_weight = [ts.ckks_vector(context, model_dict[\"model0\"].fc1.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_0_1_bias   = ts.ckks_vector(context, model_dict[\"model0\"].fc1.bias[:].data)\n",
    "encrypted_model_0_2_weight = [ts.ckks_vector(context, model_dict[\"model0\"].fc2.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_0_2_bias   = ts.ckks_vector(context, model_dict[\"model0\"].fc2.bias[:].data)\n",
    "encrypted_model_0_3_weight = [ts.ckks_vector(context, model_dict[\"model0\"].fc3.weight[i,:].data) for i in range(10)]\n",
    "encrypted_model_0_3_bias   = ts.ckks_vector(context, model_dict[\"model0\"].fc3.bias[:].data)\n",
    "\n",
    "encrypted_model_1_1_weight = [ts.ckks_vector(context, model_dict[\"model1\"].fc1.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_1_1_bias   = ts.ckks_vector(context, model_dict[\"model1\"].fc1.bias[:].data)\n",
    "encrypted_model_1_2_weight = [ts.ckks_vector(context, model_dict[\"model1\"].fc2.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_1_2_bias   = ts.ckks_vector(context, model_dict[\"model1\"].fc2.bias[:].data)\n",
    "encrypted_model_1_3_weight = [ts.ckks_vector(context, model_dict[\"model1\"].fc3.weight[i,:].data) for i in range(10)]\n",
    "encrypted_model_1_3_bias   = ts.ckks_vector(context, model_dict[\"model1\"].fc3.bias[:].data)\n",
    "\n",
    "encrypted_model_2_1_weight = [ts.ckks_vector(context, model_dict[\"model2\"].fc1.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_2_1_bias   = ts.ckks_vector(context, model_dict[\"model2\"].fc1.bias[:].data)\n",
    "encrypted_model_2_2_weight = [ts.ckks_vector(context, model_dict[\"model2\"].fc2.weight[i,:].data) for i in range(200)]\n",
    "encrypted_model_2_2_bias   = ts.ckks_vector(context, model_dict[\"model2\"].fc2.bias[:].data)\n",
    "encrypted_model_2_3_weight = [ts.ckks_vector(context, model_dict[\"model2\"].fc3.weight[i,:].data) for i in range(10)]\n",
    "encrypted_model_2_3_bias   = ts.ckks_vector(context, model_dict[\"model2\"].fc3.bias[:].data)\n",
    "\n",
    "for i in range(200):\n",
    "    encrypted_model_0_1_weight[i] = encrypted_model_0_1_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_0_1_weight_' + str(i), encrypted_model_0_1_weight[i])\n",
    "    encrypted_model_1_1_weight[i] = encrypted_model_1_1_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_1_1_weight_' + str(i), encrypted_model_1_1_weight[i])\n",
    "    encrypted_model_2_1_weight[i] = encrypted_model_2_1_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_2_1_weight_' + str(i), encrypted_model_2_1_weight[i])\n",
    "\n",
    "    encrypted_model_0_2_weight[i] = encrypted_model_0_2_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_0_2_weight_' + str(i), encrypted_model_0_2_weight[i])\n",
    "    encrypted_model_1_2_weight[i] = encrypted_model_1_2_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_1_2_weight_' + str(i), encrypted_model_1_2_weight[i])\n",
    "    encrypted_model_2_2_weight[i] = encrypted_model_2_2_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_2_2_weight_' + str(i), encrypted_model_2_2_weight[i])\n",
    "\n",
    "for i in range(10):\n",
    "    encrypted_model_0_3_weight[i] = encrypted_model_0_3_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_0_3_weight_' + str(i), encrypted_model_0_3_weight[i])\n",
    "    encrypted_model_1_3_weight[i] = encrypted_model_1_3_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_1_3_weight_' + str(i), encrypted_model_1_3_weight[i])\n",
    "    encrypted_model_2_3_weight[i] = encrypted_model_2_3_weight[i].serialize()\n",
    "    write_data('Local_Model_Parameters/encrypted_model_2_3_weight_' + str(i), encrypted_model_2_3_weight[i])\n",
    "\n",
    "encrypted_model_0_1_bias = encrypted_model_0_1_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_0_1_bias', encrypted_model_0_1_bias)\n",
    "encrypted_model_0_2_bias = encrypted_model_0_2_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_0_2_bias', encrypted_model_0_2_bias)\n",
    "encrypted_model_0_3_bias = encrypted_model_0_3_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_0_3_bias', encrypted_model_0_3_bias)\n",
    "\n",
    "encrypted_model_1_1_bias = encrypted_model_1_1_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_1_1_bias', encrypted_model_1_1_bias)\n",
    "encrypted_model_1_2_bias = encrypted_model_1_2_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_1_2_bias', encrypted_model_1_2_bias)\n",
    "encrypted_model_1_3_bias = encrypted_model_1_3_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_1_3_bias', encrypted_model_1_3_bias)\n",
    "\n",
    "encrypted_model_2_1_bias = encrypted_model_2_1_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_2_1_bias', encrypted_model_2_1_bias)\n",
    "encrypted_model_2_2_bias = encrypted_model_2_2_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_2_2_bias', encrypted_model_2_2_bias)\n",
    "encrypted_model_2_3_bias = encrypted_model_2_3_bias.serialize()\n",
    "write_data('Local_Model_Parameters/encrypted_model_2_3_bias', encrypted_model_2_3_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc7661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = dec.tolist()\n",
    "# output = np.reshape(output, [200, 200])\n",
    "# output = torch.as_tensor(output)\n",
    "# model_dict[\"model0\"].fc2.weight.data = output\n",
    "# print(model_dict[\"model0\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d7f47ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_data_from_local_models_to_server(model_dict, number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536e22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdd688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e42d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
